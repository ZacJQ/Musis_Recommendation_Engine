{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv(\"/Users/zac/Codes/Music_Project/GIT_HUB/Musis_Recommendation_Engine/exploration/Data_collection/matched_songs_with_metadata_new.csv\")\n",
    "\n",
    "dataset = dataset.rename(columns={\"MOOD\":\"mood\"})\n",
    "dataset.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "# Select relevant features\n",
    "features = ['valence', 'popularity', 'year', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'tempo', 'duration_ms']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['track_name', 'artist_name', 'mood', 'duration_ms', 'year',\n",
       "       'popularity', 'danceability', 'energy', 'key', 'loudness', 'mode',\n",
       "       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n",
       "       'valence', 'tempo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "dataset = dataset.dropna(subset=features+['mood'])\n",
    "\n",
    "# # One-hot encode 'genre' column with a prefix\n",
    "# dataset = pd.get_dummies(dataset, columns=['genre'], prefix='genre')\n",
    "\n",
    "\n",
    "# Remove rows with missing values\n",
    "dataset = dataset.dropna(subset=features+['mood'])\n",
    "dataset = dataset.drop(columns=['artist_name','track_name'])\n",
    "\n",
    "# Select features again after one-hot encoding\n",
    "features = dataset.columns.tolist()\n",
    "features.remove('mood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mood</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>year</th>\n",
       "      <th>popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAD</td>\n",
       "      <td>201080</td>\n",
       "      <td>2011</td>\n",
       "      <td>60</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.2900</td>\n",
       "      <td>4</td>\n",
       "      <td>-8.485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>166.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAD</td>\n",
       "      <td>237020</td>\n",
       "      <td>2019</td>\n",
       "      <td>65</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.2080</td>\n",
       "      <td>11</td>\n",
       "      <td>-11.917</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>169.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAD</td>\n",
       "      <td>242652</td>\n",
       "      <td>2019</td>\n",
       "      <td>75</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.0561</td>\n",
       "      <td>0</td>\n",
       "      <td>-23.023</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.3880</td>\n",
       "      <td>0.0820</td>\n",
       "      <td>79.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SAD</td>\n",
       "      <td>189634</td>\n",
       "      <td>2018</td>\n",
       "      <td>50</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.2380</td>\n",
       "      <td>10</td>\n",
       "      <td>-9.613</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0847</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.7360</td>\n",
       "      <td>152.702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SAD</td>\n",
       "      <td>291796</td>\n",
       "      <td>2019</td>\n",
       "      <td>78</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.435</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0382</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.004530</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>137.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SAD</td>\n",
       "      <td>119410</td>\n",
       "      <td>2019</td>\n",
       "      <td>68</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>6</td>\n",
       "      <td>-21.877</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>74.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SAD</td>\n",
       "      <td>203569</td>\n",
       "      <td>2017</td>\n",
       "      <td>79</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.4120</td>\n",
       "      <td>7</td>\n",
       "      <td>-8.461</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0402</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.2470</td>\n",
       "      <td>170.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SAD</td>\n",
       "      <td>200186</td>\n",
       "      <td>2018</td>\n",
       "      <td>89</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>4</td>\n",
       "      <td>-10.109</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.934000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0950</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>115.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SAD</td>\n",
       "      <td>227693</td>\n",
       "      <td>2018</td>\n",
       "      <td>70</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.249</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>117.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SAD</td>\n",
       "      <td>262760</td>\n",
       "      <td>2019</td>\n",
       "      <td>46</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.374</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.379000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>134.336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SAD</td>\n",
       "      <td>351920</td>\n",
       "      <td>2017</td>\n",
       "      <td>51</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.490</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>0.3280</td>\n",
       "      <td>95.871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SAD</td>\n",
       "      <td>208212</td>\n",
       "      <td>2015</td>\n",
       "      <td>75</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.3660</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.510</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.969000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>86.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SAD</td>\n",
       "      <td>290147</td>\n",
       "      <td>2017</td>\n",
       "      <td>81</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.4670</td>\n",
       "      <td>5</td>\n",
       "      <td>-9.018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>0.1740</td>\n",
       "      <td>94.473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SAD</td>\n",
       "      <td>179080</td>\n",
       "      <td>2018</td>\n",
       "      <td>85</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>1</td>\n",
       "      <td>-16.541</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>118.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SAD</td>\n",
       "      <td>187932</td>\n",
       "      <td>2018</td>\n",
       "      <td>84</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>4</td>\n",
       "      <td>-18.717</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>109.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SAD</td>\n",
       "      <td>182160</td>\n",
       "      <td>2019</td>\n",
       "      <td>84</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.4050</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.679</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.4460</td>\n",
       "      <td>109.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>230319</td>\n",
       "      <td>2011</td>\n",
       "      <td>59</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.5990</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.758</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.9490</td>\n",
       "      <td>116.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>236800</td>\n",
       "      <td>1958</td>\n",
       "      <td>12</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.4550</td>\n",
       "      <td>4</td>\n",
       "      <td>-7.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0551</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1540</td>\n",
       "      <td>0.8920</td>\n",
       "      <td>120.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>207307</td>\n",
       "      <td>2018</td>\n",
       "      <td>52</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.5710</td>\n",
       "      <td>2</td>\n",
       "      <td>-12.519</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1390</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.6220</td>\n",
       "      <td>167.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>229080</td>\n",
       "      <td>2015</td>\n",
       "      <td>69</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.5930</td>\n",
       "      <td>11</td>\n",
       "      <td>-4.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.4380</td>\n",
       "      <td>98.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>156886</td>\n",
       "      <td>2018</td>\n",
       "      <td>63</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.7630</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.480</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>136.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>207320</td>\n",
       "      <td>2018</td>\n",
       "      <td>50</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.6620</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.745</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.4080</td>\n",
       "      <td>107.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>181176</td>\n",
       "      <td>2021</td>\n",
       "      <td>67</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>5</td>\n",
       "      <td>-8.272</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0540</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.0740</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>102.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>180414</td>\n",
       "      <td>2019</td>\n",
       "      <td>64</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.8170</td>\n",
       "      <td>11</td>\n",
       "      <td>-6.187</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0474</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.038600</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.4830</td>\n",
       "      <td>144.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>184371</td>\n",
       "      <td>2021</td>\n",
       "      <td>56</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.828</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.5720</td>\n",
       "      <td>76.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>222096</td>\n",
       "      <td>2021</td>\n",
       "      <td>61</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.4490</td>\n",
       "      <td>4</td>\n",
       "      <td>-9.379</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.477000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5360</td>\n",
       "      <td>0.6060</td>\n",
       "      <td>100.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>230204</td>\n",
       "      <td>2019</td>\n",
       "      <td>59</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.4120</td>\n",
       "      <td>5</td>\n",
       "      <td>-8.195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3640</td>\n",
       "      <td>0.004340</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.2150</td>\n",
       "      <td>0.3560</td>\n",
       "      <td>98.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>193426</td>\n",
       "      <td>2022</td>\n",
       "      <td>60</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.3450</td>\n",
       "      <td>11</td>\n",
       "      <td>-8.464</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.3380</td>\n",
       "      <td>84.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>219240</td>\n",
       "      <td>2021</td>\n",
       "      <td>56</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.6390</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.964</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0676</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.4480</td>\n",
       "      <td>102.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>184848</td>\n",
       "      <td>2019</td>\n",
       "      <td>54</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.465</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0703</td>\n",
       "      <td>0.364000</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>98.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SCARED</td>\n",
       "      <td>142452</td>\n",
       "      <td>2019</td>\n",
       "      <td>53</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>2</td>\n",
       "      <td>-8.240</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.618000</td>\n",
       "      <td>0.0808</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>155.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SCARED</td>\n",
       "      <td>130351</td>\n",
       "      <td>2022</td>\n",
       "      <td>44</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>9</td>\n",
       "      <td>-9.164</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.6800</td>\n",
       "      <td>105.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>SCARED</td>\n",
       "      <td>132231</td>\n",
       "      <td>2020</td>\n",
       "      <td>53</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>9</td>\n",
       "      <td>-10.637</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.009810</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>90.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>SCARED</td>\n",
       "      <td>80490</td>\n",
       "      <td>2015</td>\n",
       "      <td>51</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.7460</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.435</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>0.007710</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>125.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>SCARED</td>\n",
       "      <td>60333</td>\n",
       "      <td>2021</td>\n",
       "      <td>74</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>9</td>\n",
       "      <td>-11.313</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0414</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.5510</td>\n",
       "      <td>119.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SCARED</td>\n",
       "      <td>111711</td>\n",
       "      <td>2020</td>\n",
       "      <td>51</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.2420</td>\n",
       "      <td>10</td>\n",
       "      <td>-10.066</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>0.879000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3940</td>\n",
       "      <td>0.4090</td>\n",
       "      <td>75.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>SCARED</td>\n",
       "      <td>197733</td>\n",
       "      <td>2012</td>\n",
       "      <td>71</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.4670</td>\n",
       "      <td>4</td>\n",
       "      <td>-9.558</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.4020</td>\n",
       "      <td>105.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ANGRY</td>\n",
       "      <td>231400</td>\n",
       "      <td>2006</td>\n",
       "      <td>72</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.8530</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.668</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>122.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ANGRY</td>\n",
       "      <td>247426</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.7830</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.697</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.1610</td>\n",
       "      <td>0.6430</td>\n",
       "      <td>124.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ANGRY</td>\n",
       "      <td>292400</td>\n",
       "      <td>2011</td>\n",
       "      <td>9</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.9290</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.738</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.006970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3870</td>\n",
       "      <td>0.6040</td>\n",
       "      <td>86.505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ANGRY</td>\n",
       "      <td>188960</td>\n",
       "      <td>2004</td>\n",
       "      <td>71</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.7390</td>\n",
       "      <td>9</td>\n",
       "      <td>-5.354</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.3820</td>\n",
       "      <td>130.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ANGRY</td>\n",
       "      <td>325460</td>\n",
       "      <td>2019</td>\n",
       "      <td>32</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0415</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3190</td>\n",
       "      <td>0.5770</td>\n",
       "      <td>99.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>ANGRY</td>\n",
       "      <td>467587</td>\n",
       "      <td>2015</td>\n",
       "      <td>74</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.875</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0443</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.5770</td>\n",
       "      <td>125.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ANGRY</td>\n",
       "      <td>238200</td>\n",
       "      <td>2017</td>\n",
       "      <td>64</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.877</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>135.902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mood  duration_ms  year  popularity  danceability  energy  key  \\\n",
       "0      SAD       201080  2011          60         0.379  0.2900    4   \n",
       "1      SAD       237020  2019          65         0.453  0.2080   11   \n",
       "2      SAD       242652  2019          75         0.319  0.0561    0   \n",
       "3      SAD       189634  2018          50         0.528  0.2380   10   \n",
       "4      SAD       291796  2019          78         0.421  0.1310    0   \n",
       "5      SAD       119410  2019          68         0.153  0.1380    6   \n",
       "6      SAD       203569  2017          79         0.483  0.4120    7   \n",
       "7      SAD       200186  2018          89         0.351  0.2960    4   \n",
       "8      SAD       227693  2018          70         0.481  0.2670    7   \n",
       "9      SAD       262760  2019          46         0.339  0.4950    5   \n",
       "10     SAD       351920  2017          51         0.368  0.2370    0   \n",
       "11     SAD       208212  2015          75         0.545  0.3660    2   \n",
       "12     SAD       290147  2017          81         0.369  0.4670    5   \n",
       "13     SAD       179080  2018          85         0.498  0.2350    1   \n",
       "14     SAD       187932  2018          84         0.522  0.1280    4   \n",
       "15     SAD       182160  2019          84         0.501  0.4050    1   \n",
       "16   HAPPY       230319  2011          59         0.778  0.5990    2   \n",
       "17   HAPPY       236800  1958          12         0.563  0.4550    4   \n",
       "18   HAPPY       207307  2018          52         0.657  0.5710    2   \n",
       "19   HAPPY       229080  2015          69         0.672  0.5930   11   \n",
       "20   HAPPY       156886  2018          63         0.804  0.7630    2   \n",
       "21   HAPPY       207320  2018          50         0.743  0.6620    1   \n",
       "22   HAPPY       181176  2021          67         0.786  0.6670    5   \n",
       "23   HAPPY       180414  2019          64         0.595  0.8170   11   \n",
       "24   HAPPY       184371  2021          56         0.461  0.5000    1   \n",
       "25   HAPPY       222096  2021          61         0.678  0.4490    4   \n",
       "26   HAPPY       230204  2019          59         0.663  0.4120    5   \n",
       "27   HAPPY       193426  2022          60         0.659  0.3450   11   \n",
       "28   HAPPY       219240  2021          56         0.833  0.6390    5   \n",
       "29   HAPPY       184848  2019          54         0.787  0.7340    0   \n",
       "30  SCARED       142452  2019          53         0.454  0.7250    2   \n",
       "31  SCARED       130351  2022          44         0.705  0.5000    9   \n",
       "32  SCARED       132231  2020          53         0.507  0.3750    9   \n",
       "33  SCARED        80490  2015          51         0.587  0.7460    2   \n",
       "34  SCARED        60333  2021          74         0.680  0.4060    9   \n",
       "35  SCARED       111711  2020          51         0.641  0.2420   10   \n",
       "36  SCARED       197733  2012          71         0.693  0.4670    4   \n",
       "37   ANGRY       231400  2006          72         0.553  0.8530    8   \n",
       "38   ANGRY       247426  2013           6         0.532  0.7830    2   \n",
       "39   ANGRY       292400  2011           9         0.241  0.9290    7   \n",
       "40   ANGRY       188960  2004          71         0.662  0.7390    9   \n",
       "41   ANGRY       325460  2019          32         0.499  0.4800    1   \n",
       "42   ANGRY       467587  2015          74         0.602  0.8810    1   \n",
       "43   ANGRY       238200  2017          64         0.424  0.5060    2   \n",
       "\n",
       "    loudness  mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
       "0     -8.485     1       0.0510      0.952000          0.001060    0.1180   \n",
       "1    -11.917     1       0.2200      0.585000          0.000000    0.1190   \n",
       "2    -23.023     1       0.0450      0.935000          0.003840    0.3880   \n",
       "3     -9.613     1       0.0847      0.620000          0.000000    0.1200   \n",
       "4    -18.435     1       0.0382      0.952000          0.004530    0.1090   \n",
       "5    -21.877     0       0.0503      0.837000          0.550000    0.2540   \n",
       "6     -8.461     1       0.0402      0.737000          0.000000    0.1160   \n",
       "7    -10.109     0       0.0333      0.934000          0.000000    0.0950   \n",
       "8     -9.249     1       0.0339      0.800000          0.000000    0.2490   \n",
       "9     -6.374     1       0.0325      0.379000          0.000000    0.1020   \n",
       "10    -9.490     1       0.0389      0.792000          0.000000    0.1220   \n",
       "11    -9.510     1       0.0378      0.969000          0.267000    0.6410   \n",
       "12    -9.018     1       0.0274      0.019400          0.460000    0.1090   \n",
       "13   -16.541     0       0.1880      0.720000          0.000000    0.1440   \n",
       "14   -18.717     1       0.0357      0.894000          0.026000    0.0941   \n",
       "15    -5.679     1       0.0319      0.751000          0.000000    0.1050   \n",
       "16    -4.758     1       0.0320      0.070500          0.000000    0.5900   \n",
       "17    -7.010     0       0.0551      0.808000          0.000000    0.1540   \n",
       "18   -12.519     1       0.1390      0.110000          0.001230    0.0735   \n",
       "19    -4.010     0       0.0304      0.022300          0.000000    0.2140   \n",
       "20    -5.480     1       0.0519      0.067800          0.000000    0.3680   \n",
       "21    -5.745     1       0.0534      0.268000          0.000001    0.1020   \n",
       "22    -8.272     1       0.0540      0.011200          0.053000    0.0740   \n",
       "23    -6.187     0       0.0474      0.000712          0.038600    0.1420   \n",
       "24    -9.828     1       0.0614      0.002450          0.030000    0.1160   \n",
       "25    -9.379     0       0.0869      0.477000          0.000000    0.5360   \n",
       "26    -8.195     0       0.3640      0.004340          0.019000    0.2150   \n",
       "27    -8.464     1       0.1070      0.106000          0.000000    0.1070   \n",
       "28    -6.964     1       0.0676      0.152000          0.000000    0.1010   \n",
       "29    -6.465     1       0.0703      0.364000          0.000995    0.0625   \n",
       "30    -8.240     1       0.0343      0.126000          0.618000    0.0808   \n",
       "31    -9.164     1       0.0277      0.351000          0.017200    0.2370   \n",
       "32   -10.637     1       0.0256      0.855000          0.009810    0.1100   \n",
       "33    -5.435     1       0.0388      0.007710          0.000022    0.1440   \n",
       "34   -11.313     1       0.0414      0.782000          0.000190    0.1100   \n",
       "35   -10.066     1       0.0757      0.879000          0.000000    0.3940   \n",
       "36    -9.558     0       0.0430      0.049700          0.452000    0.1070   \n",
       "37    -3.668     1       0.0339      0.000684          0.000000    0.1260   \n",
       "38    -5.697     1       0.0523      0.003800          0.001200    0.1610   \n",
       "39    -4.738     1       0.0546      0.006970          0.000000    0.3870   \n",
       "40    -5.354     0       0.0322      0.002060          0.060300    0.1130   \n",
       "41    -6.450     1       0.0415      0.619000          0.000000    0.3190   \n",
       "42    -5.875     0       0.0443      0.004620          0.026600    0.1110   \n",
       "43    -5.877     1       0.0284      0.201000          0.000000    0.1100   \n",
       "\n",
       "    valence    tempo  \n",
       "0    0.1690  166.467  \n",
       "1    0.2180  169.326  \n",
       "2    0.0820   79.764  \n",
       "3    0.7360  152.702  \n",
       "4    0.1200  137.446  \n",
       "5    0.0503   74.318  \n",
       "6    0.2470  170.163  \n",
       "7    0.1200  115.284  \n",
       "8    0.1370  117.640  \n",
       "9    0.1210  134.336  \n",
       "10   0.3280   95.871  \n",
       "11   0.1000   86.997  \n",
       "12   0.1740   94.473  \n",
       "13   0.0895  118.917  \n",
       "14   0.1240  109.986  \n",
       "15   0.4460  109.891  \n",
       "16   0.9490  116.951  \n",
       "17   0.8920  120.784  \n",
       "18   0.6220  167.958  \n",
       "19   0.4380   98.020  \n",
       "20   0.0585  136.052  \n",
       "21   0.4080  107.027  \n",
       "22   0.6880  102.046  \n",
       "23   0.4830  144.961  \n",
       "24   0.5720   76.176  \n",
       "25   0.6060  100.033  \n",
       "26   0.3560   98.023  \n",
       "27   0.3380   84.761  \n",
       "28   0.4480  102.989  \n",
       "29   0.6440   98.989  \n",
       "30   0.1290  155.000  \n",
       "31   0.6800  105.016  \n",
       "32   0.0519   90.006  \n",
       "33   0.3130  125.429  \n",
       "34   0.5510  119.901  \n",
       "35   0.4090   75.990  \n",
       "36   0.4020  105.958  \n",
       "37   0.5060  122.023  \n",
       "38   0.6430  124.080  \n",
       "39   0.6040   86.505  \n",
       "40   0.3820  130.999  \n",
       "41   0.5770   99.886  \n",
       "42   0.5770  125.012  \n",
       "43   0.1380  135.902  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zac/anaconda3/envs/model_music/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=1, hidden_layer_sizes=100, learning_rate=&#x27;adaptive&#x27;,\n",
       "              learning_rate_init=1e-05, max_iter=100, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=1, hidden_layer_sizes=100, learning_rate=&#x27;adaptive&#x27;,\n",
       "              learning_rate_init=1e-05, max_iter=100, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=1, hidden_layer_sizes=100, learning_rate='adaptive',\n",
       "              learning_rate_init=1e-05, max_iter=100, random_state=42)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Split dataset into features and target\n",
    "X = dataset[features]\n",
    "y = dataset['mood']\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize MLP Classifier (Neural Network)\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(100), activation='relu', solver='adam', random_state=42, max_iter=100, batch_size=1, learning_rate=\"adaptive\", learning_rate_init=1e-5)\n",
    "\n",
    "# Train classifier\n",
    "classifier.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of each class for the first few samples in the test set:\n",
      "[[0.2242936  0.39489349 0.15620471 0.2246082 ]\n",
      " [0.18465953 0.21741124 0.24710767 0.35082156]\n",
      " [0.207467   0.2151595  0.31073495 0.26663856]\n",
      " [0.37288085 0.27697883 0.17792612 0.1722142 ]\n",
      " [0.12305252 0.18248275 0.29731127 0.39715346]]\n",
      "\n",
      "Actual classes for comparison:\n",
      "['HAPPY' 'SCARED' 'SAD' 'ANGRY' 'SCARED']\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGRY       0.50      0.50      0.50         2\n",
      "       HAPPY       0.00      0.00      0.00         2\n",
      "         SAD       0.75      1.00      0.86         3\n",
      "      SCARED       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.56         9\n",
      "   macro avg       0.44      0.50      0.46         9\n",
      "weighted avg       0.47      0.56      0.51         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict probabilities on test set\n",
    "y_pred_proba = classifier.predict_proba(X_test_scaled)\n",
    "\n",
    "# Print probabilities of each class\n",
    "print(\"Probabilities of each class for the first few samples in the test set:\")\n",
    "print(y_pred_proba[:5])  # printing first 5 samples for illustration purposes\n",
    "\n",
    "# Optionally, you can print the actual classes as well for comparison\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "print(\"\\nActual classes for comparison:\")\n",
    "print(y_pred[:5])  # printing first 5 samples for illustration purposes\n",
    "\n",
    "# Evaluate classifier (optional)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HAPPY' 'SCARED' 'SAD' 'ANGRY' 'SCARED' 'ANGRY' 'SAD' 'ANGRY' 'SAD'] \n",
      " 37     ANGRY\n",
      "24     HAPPY\n",
      "25     HAPPY\n",
      "36    SCARED\n",
      "34    SCARED\n",
      "40     ANGRY\n",
      "4        SAD\n",
      "12       SAD\n",
      "8        SAD\n",
      "Name: mood, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y_pred, \"\\n\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump, load  # Added import for joblib\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv(\"/Users/zac/Codes/Music_Project/GIT_HUB/Musis_Recommendation_Engine/data/testing/testing.csv\")\n",
    "\n",
    "# Select relevant features\n",
    "features = ['valence', 'popularity', 'year', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'tempo', 'duration_ms', 'time_signature']\n",
    "\n",
    "# Remove rows with missing values\n",
    "dataset = dataset.dropna(subset=features+['mood'])\n",
    "\n",
    "# One-hot encode 'genre' column with a prefix\n",
    "dataset = pd.get_dummies(dataset, columns=['genre'], prefix='genre')\n",
    "\n",
    "# Remove rows with missing values\n",
    "dataset = dataset.dropna(subset=features+['mood'])\n",
    "dataset = dataset.drop(columns=['artist_name','track_name','track_id'])\n",
    "\n",
    "# Select features again after one-hot encoding\n",
    "features = dataset.columns.tolist()\n",
    "features.remove('mood')\n",
    "\n",
    "# Split dataset into features and target\n",
    "X = dataset[features]\n",
    "y = dataset['mood']\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.00      0.00      0.00         6\n",
      "       Happy       0.23      1.00      0.37         5\n",
      "         Sad       0.00      0.00      0.00         3\n",
      "      Scared       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.23        22\n",
      "   macro avg       0.06      0.25      0.09        22\n",
      "weighted avg       0.05      0.23      0.08        22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zac/anaconda3/envs/model_music/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zac/anaconda3/envs/model_music/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zac/anaconda3/envs/model_music/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize MLP Classifier (Neural Network)\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(100,50), activation='relu', solver='adam',max_iter=20 , random_state=42)\n",
    "\n",
    "# Train classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "dump(classifier, 'model.joblib')\n",
    "\n",
    "# Optionally, you can load the model later for inference\n",
    "# loaded_model = load('model.joblib')\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate classifier\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Happy' 'Happy' 'Happy' 'Happy' 'Happy' 'Happy' 'Happy' 'Happy' 'Happy'\n",
      " 'Happy' 'Happy' 'Happy' 'Happy' 'Happy' 'Happy' 'Happy' 'Happy' 'Happy'\n",
      " 'Happy' 'Happy' 'Happy' 'Happy']\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of each class for the first few samples in the test set:\n",
      "[[0.24216252 0.23561062 0.25799503 0.26423182]\n",
      " [0.18750752 0.27860416 0.17565757 0.35823075]\n",
      " [0.12603631 0.32724933 0.25366087 0.29305349]\n",
      " [0.15777178 0.32646337 0.18551635 0.3302485 ]\n",
      " [0.17885239 0.26088643 0.30534394 0.25491724]]\n",
      "\n",
      "Actual classes for comparison:\n",
      "['Scared' 'Scared' 'Happy' 'Scared' 'Sad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zac/anaconda3/envs/model_music/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/zac/anaconda3/envs/model_music/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities on test set\n",
    "y_pred_proba = classifier.predict_proba(X_test_scaled)\n",
    "\n",
    "# Print probabilities of each class\n",
    "print(\"Probabilities of each class for the first few samples in the test set:\")\n",
    "print(y_pred_proba[:5])  # printing first 5 samples for illustration purposes\n",
    "\n",
    "# Optionally, you can print the actual classes as well for comparison\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "print(\"\\nActual classes for comparison:\")\n",
    "print(y_pred[:5])  # printing first 5 samples for illustration purposes\n",
    "\n",
    "# # Evaluate classifier (optional)\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
